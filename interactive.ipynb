{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(\"/opt/project\")\n",
    "\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from contextlib import suppress\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "\n",
    "import keras\n",
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "from model import get_model\n",
    "from datasets import ECGSequence"
   ],
   "id": "dad4f1dcc567e03a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LOGS_PATH = Path(\"logs\")\n",
    "MODELS_PATH = Path(\"models\")\n",
    "FIGURES_PATH = Path(\"figures\")\n",
    "PREDICTIONS_PATH = Path(\"predictions\")\n",
    "\n",
    "TEST_DATA_PATH = Path(\"data/test\")\n",
    "INPUT_DATA_PATH = Path(\"data/train/inputs\")\n",
    "LABEL_DATA_PATH = Path(\"data/train/labels\")\n",
    "TEST_ANNOTATIONS_PATH = TEST_DATA_PATH / \"annotations\"\n",
    "\n",
    "input_files = os.listdir(INPUT_DATA_PATH)\n",
    "label_files = os.listdir(LABEL_DATA_PATH)"
   ],
   "id": "b38c6893e256b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def align_entries(indices: np.ndarray, data: np.ndarray, num_classes=6):\n",
    "    ydict = {}\n",
    "    for item in data:\n",
    "        ydict[item[0]] = item\n",
    "\n",
    "    result = np.empty(shape=(len(indices), num_classes), dtype=object)\n",
    "    for index, value in enumerate(indices):\n",
    "        with suppress(KeyError):\n",
    "            result[index] = ydict[str(value)][4: 4 + num_classes] == \"True\"\n",
    "\n",
    "    return result"
   ],
   "id": "9e2909f709d4734d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_and_export_file_data(input_file: str, output_file: str):\n",
    "    y = pd.read_csv(LABEL_DATA_PATH.parent / \"exams.csv\", dtype=object).values\n",
    "    with h5py.File(INPUT_DATA_PATH / f\"{input_file}\", \"r+\") as file:\n",
    "        x, ids = file[\"tracings\"], file[\"exam_id\"]\n",
    "        y_curr = align_entries(ids, y)\n",
    "\n",
    "        # Log info\n",
    "        print(f\" FILE: {input_file} -> {output_file} \".center(60, \"*\"))\n",
    "        print(f\"X SHAPE: {x.shape}\".center(60, \" \"))\n",
    "        print(f\"Y SHAPE: {y_curr.shape}\".center(60, \" \"))\n",
    "        print(f\"I SHAPE: {ids.shape}\\n\".center(60, \" \"))\n",
    "\n",
    "        # Save labels\n",
    "        pd.DataFrame(y_curr).astype(int).to_csv(\n",
    "            LABEL_DATA_PATH / output_file,\n",
    "            sep=\",\", encoding=\"utf-8\", index=False, header=True\n",
    "        )"
   ],
   "id": "fb031cf915db61d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(input_files)):\n",
    "    if not f\"exams_part{i}.hdf5\" in input_files: break\n",
    "    process_and_export_file_data(f\"exams_part{i}.hdf5\", f\"exams_part{i}.csv\")"
   ],
   "id": "b8f419cebccb690d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Model settings\n",
    "val_split = 0.02\n",
    "dataset_name = \"tracings\"\n",
    "model_id = len(os.listdir(\"models\")) - 1\n",
    "worker_num = psutil.cpu_count(logical=True) - 2\n",
    "\n",
    "# Optimization settings\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "opt = keras.optimizers.Adam(lr)\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "callbacks = [\n",
    "    # Learning Optimizers\n",
    "    keras.callbacks.EarlyStopping(patience=9, min_delta=0.00001),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1,\n",
    "                                      patience=7, min_lr=lr / 100),\n",
    "    # Logs\n",
    "    keras.callbacks.TensorBoard(log_dir=LOGS_PATH, write_graph=False),\n",
    "    keras.callbacks.CSVLogger(LOGS_PATH / \"training.log\", append=False),\n",
    "    # Checkpoints\n",
    "    keras.callbacks.ModelCheckpoint(MODELS_PATH / f\"backup/model_last_{model_id}.keras\"),\n",
    "    keras.callbacks.ModelCheckpoint(MODELS_PATH / f\"backup/model_best_{model_id}.keras\"),\n",
    "]\n",
    "\n",
    "train_seq, valid_seq = ECGSequence.get_train_and_val(\n",
    "    [INPUT_DATA_PATH / file for file in input_files],\n",
    "    [LABEL_DATA_PATH / file for file in label_files],\n",
    "    dataset_name, batch_size, val_split,\n",
    "    workers=worker_num, use_multiprocessing=True\n",
    ")\n",
    "\n",
    "# If you are continuing an interrupted section, uncomment line bellow:\n",
    "# model = keras.models.load_model(PATH_TO_MODEL, compile=False)\n",
    "model = get_model(train_seq.n_classes)\n",
    "model.compile(loss=loss, optimizer=opt)"
   ],
   "id": "f129812630aea808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train neural network\n",
    "# If you are continuing an interrupted section change initial epoch\n",
    "history = model.fit(train_seq,\n",
    "                    epochs=70,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_seq,\n",
    "                    verbose=1)\n",
    "# Save final result\n",
    "model.save(MODELS_PATH / f\"model_{model_id}.keras\")"
   ],
   "id": "2c6c200cb4d7bd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predictions of the model on the test set\n",
    "seq = ECGSequence([TEST_DATA_PATH / \"ecg_tracings.hdf5\"], None, dataset_name, batch_size=1)\n",
    "y_score = model.predict(seq, verbose=1)\n",
    "\n",
    "np.save(PREDICTIONS_PATH / f\"predictions_{model_id}.npy\", y_score)"
   ],
   "id": "ae14a1cda51393c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_id = 1",
   "id": "8d7206697e737ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def get_scores(y_true, y_nn, score_func):\n",
    "    scores_arr = []\n",
    "    for name, fun in score_func.items():\n",
    "        scores_arr += [[fun(y_true[:, k], y_nn[:, k])\n",
    "                    for k in range(np.shape(y_true)[1])]]\n",
    "    return np.array(scores_arr).T\n",
    "\n",
    "\n",
    "def get_optimal_precision_recall(y_true, y_nn):\n",
    "    \"\"\"Find precision and recall values that maximize f1 score.\"\"\"\n",
    "    opt_recall = []\n",
    "    opt_precision = []\n",
    "    opt_threshold = []\n",
    "    for k in range(np.shape(y_true)[1]):\n",
    "        # Get precision-recall curve\n",
    "        precision, recall, thresh = precision_recall_curve(y_true[:, k], y_nn[:, k])\n",
    "        f1 = np.nan_to_num(2 * precision * recall / (precision + recall))\n",
    "\n",
    "        # Select threshold that maximize f1 score\n",
    "        f1_index = np.argmax(f1)\n",
    "\n",
    "        opt_recall.append(recall[f1_index])\n",
    "        opt_precision.append(precision[f1_index])\n",
    "\n",
    "        t = thresh[f1_index - 1] if f1_index != 0 else thresh[0] - 1e-10\n",
    "        opt_threshold.append(t)\n",
    "\n",
    "    return np.array(opt_precision), np.array(opt_recall), np.array(opt_threshold)"
   ],
   "id": "683456de9863e2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "score_fun = {\"Precision\": precision_score, \"Recall\": recall_score,\n",
    "             \"Specificity\": partial(recall_score, pos_label=0), \"F1 score\": f1_score}\n",
    "\n",
    "predictor_names = [\"DNN\", \"cardio.\", \"emerg.\", \"stud.\"]\n",
    "diagnosis = [\"1dAVb\", \"RBBB\", \"LBBB\", \"SB\", \"AF\", \"ST\"]\n",
    "nclasses = len(diagnosis)"
   ],
   "id": "e42772cb2b6c7313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read datasets\n",
    "\n",
    "# Get cardiologists, residents and students performance\n",
    "y_gold = pd.read_csv(TEST_ANNOTATIONS_PATH / \"gold_standard.csv\").values\n",
    "y_student = pd.read_csv(TEST_ANNOTATIONS_PATH / \"medical_students.csv\").values\n",
    "y_emerg = pd.read_csv(TEST_ANNOTATIONS_PATH / \"emergency_residents.csv\").values\n",
    "y_cardio = pd.read_csv(TEST_ANNOTATIONS_PATH / \"cardiology_residents.csv\").values\n",
    "\n",
    "y_dnn = np.load(PREDICTIONS_PATH / f\"prediction_{model_id}.npy\")"
   ],
   "id": "2eec67dd3e06956a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get average model\n",
    "\n",
    "# Get micro average precision\n",
    "micro_avg_precision = average_precision_score(y_gold[:, :6], y_dnn[:, :6], average=\"micro\")\n",
    "print(f\"Micro average precision: {micro_avg_precision}\")\n",
    "\n",
    "# Get threshold that yield the best precision recall using\n",
    "# \"get_optimal_precision_recall\" on validation set\n",
    "threshold = np.array([0.124, 0.07, 0.05, 0.278, 0.390, 0.174])\n",
    "mask = y_dnn > threshold\n",
    "\n",
    "# Get neural network prediction\n",
    "y_neuralnet = np.zeros_like(y_dnn)\n",
    "y_neuralnet[mask] = 1"
   ],
   "id": "1316e6a7abc8d35e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate table with scores for the average model (Table 2)\n",
    "\n",
    "scores_list = []\n",
    "for y_pred in [y_neuralnet, y_cardio, y_emerg, y_student]:\n",
    "    scores = get_scores(y_gold, y_pred, score_fun)\n",
    "    scores_df = pd.DataFrame(scores, index=diagnosis, columns=score_fun.keys())\n",
    "    scores_list.append(scores_df)\n",
    "\n",
    "# Concatenate dataframes\n",
    "scores_all_df = pd.concat(scores_list, axis=1, keys=predictor_names)\n",
    "scores_all_df = scores_all_df.swaplevel(0, 1, axis=1)\n",
    "scores_all_df = scores_all_df.reindex(level=0, columns=score_fun.keys())\n",
    "\n",
    "# Save results\n",
    "scores_all_df.to_csv(FIGURES_PATH / f\"scores_{model_id}.csv\", float_format=\"%.3f\")\n"
   ],
   "id": "b7c3bf1e144aade3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute scores and bootstrapped version of these scores\n",
    "\n",
    "bootstrap_nsamples = 1000\n",
    "percentiles = [2.5, 97.5]\n",
    "scores_resampled_list = []\n",
    "scores_percentiles_list = []\n",
    "for y_pred in [y_neuralnet, y_cardio, y_emerg, y_student]:\n",
    "    # Compute bootstrapped samples\n",
    "    np.random.seed(123)  # NEVER change this\n",
    "    n, _ = np.shape(y_gold)\n",
    "    samples = np.random.randint(n, size=n * bootstrap_nsamples)\n",
    "\n",
    "    # Get samples\n",
    "    y_true_resampled = np.reshape(y_gold[samples, :], (bootstrap_nsamples, n, nclasses))\n",
    "    y_doctors_resampled = np.reshape(y_pred[samples, :], (bootstrap_nsamples, n, nclasses))\n",
    "\n",
    "    # Apply functions\n",
    "    scores_resampled = np.array(\n",
    "        [get_scores(y_true_resampled[i, :, :], y_doctors_resampled[i, :, :], score_fun)\n",
    "         for i in range(bootstrap_nsamples)]\n",
    "    )\n",
    "\n",
    "    # Sort and append scores\n",
    "    scores_resampled.sort(axis=0)\n",
    "    scores_resampled_list.append(scores_resampled)\n",
    "\n",
    "    # Get percentiles\n",
    "    i = [int(p / 100.0 * bootstrap_nsamples) for p in percentiles]\n",
    "    scores_percentiles = scores_resampled[i, :, :]\n",
    "    scores_percentiles_df = pd.concat([pd.DataFrame(x, index=diagnosis, columns=score_fun.keys())\n",
    "                                       for x in scores_percentiles], keys=[\"p1\", \"p2\"], axis=1)\n",
    "    scores_percentiles_df = scores_percentiles_df.swaplevel(0, 1, axis=1)\n",
    "    scores_percentiles_df = scores_percentiles_df.reindex(level=0, columns=score_fun.keys())\n",
    "    scores_percentiles_list.append(scores_percentiles_df)\n",
    "\n",
    "# Concatenate dataframes\n",
    "scores_percentiles_all_df = pd.concat(scores_percentiles_list, axis=1, keys=predictor_names)\n",
    "scores_percentiles_all_df = scores_percentiles_all_df.reorder_levels([1, 0, 2], axis=1)\n",
    "scores_percentiles_all_df = scores_percentiles_all_df.reindex(level=0, columns=score_fun.keys())"
   ],
   "id": "eaca0ab4e8a146e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print box plot (Supplementary Figure 1)\n",
    "\n",
    "# Convert to xarray\n",
    "scores_resampled_xr = xr.DataArray(\n",
    "    np.array(scores_resampled_list),\n",
    "    dims=[\"predictor\", \"n\", \"diagnosis\", \"score_fun\"],\n",
    "    coords={\n",
    "        \"predictor\": predictor_names,\n",
    "        \"n\": range(bootstrap_nsamples),\n",
    "        \"score_fun\": list(score_fun.keys()),\n",
    "        \"diagnosis\": diagnosis,\n",
    "    }\n",
    ")\n",
    "\n",
    "for sf in score_fun:\n",
    "    fig, ax = plt.subplots()\n",
    "    f1_score_resampled_xr = scores_resampled_xr.sel(score_fun=sf)\n",
    "    f1_score_resampled_df = f1_score_resampled_xr.to_dataframe(name=sf).reset_index(level=[0, 1, 2])\n",
    "\n",
    "    # Plot seaborn\n",
    "    sns.boxplot(x=\"diagnosis\", y=sf, hue=\"predictor\", data=f1_score_resampled_df, ax=ax)\n",
    "\n",
    "    # Save results\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\", fontsize=16)\n",
    "\n",
    "    if sf == \"F1 score\":\n",
    "        plt.legend(fontsize=17)\n",
    "    else:\n",
    "        ax.legend().remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_PATH / f\"boxplot_bootstrap_{sf.lower()}_{model_id}.pdf\")\n",
    "\n",
    "(scores_resampled_xr.to_dataframe(name=\"score\")\n",
    " .to_csv(FIGURES_PATH / f\"boxplot_bootstrap_data_{model_id}.txt\"))\n"
   ],
   "id": "56571ad38592f6fc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
